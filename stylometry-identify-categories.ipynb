{"cells":[{"cell_type":"markdown","source":["# Stylometry - or: Identify authors by sentence structure using \"deep learning\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a383545b-f960-4c82-ad85-64f0f2d52ef1"}}},{"cell_type":"markdown","source":["## Introduction & Features\nWe live in changing times: With the way our social media activity increases, we now have a shift away from traditional communication towards a text-based one. While these type of messages may have significant advantages and allows us to persist in a globalized world, some question and problem arise we have to face. Undoubtfully, one crucial ability is the identification of an author through its texts; without direct contact, fakes are elsewhere hardly recognizable.\nThe science of identifying authors by there writing is called Stylometry. With our ability to access big data and having sufficient computational power nowadays, the accuracy we may gain in such kinds of tasks is quite impressive.\n\n\n![An analysis by Ali Arsalan Kazmi showing the two authors of the speeches of the Prime Minister of Pakistan ](http://aliarsalankazmi.github.io/blog_DA/assets/img/bcn_char4g_2.png)\n*A study by Ali Arsalan Kazmi identifying the two authors of the speeches of the Prime Minister of Pakistan*\n\n\nIn this kernel, instead of classifying the texts we will use some Stylometry to identify the three authors of different horror stories. Every data mining algorithm is only as good as the features and data it is working on, so we might have a first look at the available features for such kind of analysis (according to \"Writeprints: A Stylometric Approach to Identity-Level Identification and Similarity Detection in Cyberspace\" by A. ABBASI)\n\n* Lexical features: Lexical features describes the semantics of the words used by the author. Vocabulary richness and word-length distributions are classic examples.\n* Syntactic features: Such kind of features rely on an analysis of typical sentence structure considering elements like function words, punctuation, and part-of-speech tags.\n* Structural features: Structural features targets the actual composition of the text discussing its organization and layout. Due to the short extracts without any further information, this seems not applicable here.\n* Content-specific features: This type of features based on an analysis of essential keywords and phrases on specific topics like in scientific publication. Like the feature set before not applicable in our context.\n* Idiosyncratic features: Individual usage anomalies like misspellings and grammatical mistakes are taken into consideration. After the editing process of all the texts, it is unlikely that any valuable results remain.\n\nIn short, we seem to have the choice between a lexical and a syntactic approach. In a productive situation, one would probably focus on the first one; it is proven they usually performed far better than the pure sentence structure. So, if you are interested in such an analysis and gaining optimal performance, please take a look into all the other amazing kernels out there. In the following, we will focus on a more scientific question:\n\n**Utilizing Deep learning technology, is it possible to identify an author just by the structure of the sentences he or she writes?**\n\n## Tools\nDuring our journey into data science, we will utilize a wide range of the epic tools freely available for Python. In the following, I just want to go through them briefly.\n\n#### Data management: *Pandas*\nPandas is a library for loading and modification of different data structures like series and tables. Especially its wide abilities for loading and saving into different formats will be utilized in the following.\n\n#### Natural Language Processing: *SpaCy*\nWhen the terms \"Python,\" \"text intelligence,\" and \"deep learning\" are mentioned in one sentence, often the name \"SpaCy\" is used, too. The rather young project wants to be a performant alternative to NLTK and provided advanced features like a GloVe vectorization of words out of the box. All these features are not needed for our current experiment but probably more useful when targetting lexical features.  Personally, I just prefer SpaCy's style of handling problems, but NLTK would probably lead to the same results in that particular application as it used a variation of SpaCy's POS tagger. Feel free to use whatever tool you want.\n\n#### Array handling: *Numpy*\nNumpy is one of THE tools in Python's machine learning universe. It allows the efficient handling of huge multidimensional arrays with minimal copy costs and memory footprint. Depending on the style of usage, the performance benefits in comparison with Python lists are in general quite impressive. \n\n#### Neuronal network layout: *Keras*\nKeras is a wonderful simple wrapper for designing and training neural networks using the performance of the far more complex frameworks Theano and TensorFlow. With its ability to add simply one layer after the other, it is an ideal starting point for own experiments with deep learning.\n\n#### Swiss army knife for almost anything else: *scikit-learn*\nLast but not least one of the essential library for classical machine learning in Python. Independently if you want to use a PCA, a 10-fold cross validation, or a data pipeline: There is already a class for that. Its data-driven pipelines utilizing transformer and classifier will guide us through our tour.\n\nFirst of all, we import all the necessary stuff we will need in the following. After the loading of training data, we convert the names of the authors into strings for an easier further processing and load the pre-computed models SpaCy relies on."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77a0e138-f5a7-4a90-b73a-235c1970a5eb"}}},{"cell_type":"code","source":["pip install spacy && python -m spacy download en"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f1f163e-b753-4721-bee1-a01563fef38d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nRequirement already satisfied: spacy in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (2.3.2)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (2.0.4)\nRequirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (1.0.4)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (1.0.4)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (2.22.0)\nRequirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (0.4.1)\nRequirement already satisfied: numpy&gt;=1.15.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (1.18.1)\nRequirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (0.8.0)\nRequirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (1.1.3)\nRequirement already satisfied: thinc==7.4.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (7.4.1)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (3.0.4)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (4.42.1)\nRequirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (1.0.0)\nRequirement already satisfied: setuptools in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (45.2.0.post20200210)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (1.25.8)\nRequirement already satisfied: idna&lt;2.9,&gt;=2.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (2.8)\nRequirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (3.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (2020.6.20)\nRequirement already satisfied: importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34; in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy) (3.1.0)\nRequirement already satisfied: zipp&gt;=0.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34;-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy) (3.4.0)\nCollecting en_core_web_sm==2.3.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\nRequirement already satisfied: spacy&lt;2.4.0,&gt;=2.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (2.0.4)\nRequirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.0.4)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.0.4)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (2.22.0)\nRequirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (0.4.1)\nRequirement already satisfied: numpy&gt;=1.15.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.18.1)\nRequirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (0.8.0)\nRequirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.1.3)\nRequirement already satisfied: thinc==7.4.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (7.4.1)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (3.0.4)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (4.42.1)\nRequirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.0.0)\nRequirement already satisfied: setuptools in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (45.2.0.post20200210)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.25.8)\nRequirement already satisfied: idna&lt;2.9,&gt;=2.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (2.8)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (2020.6.20)\nRequirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (3.0.4)\nRequirement already satisfied: importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34; in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (3.1.0)\nRequirement already satisfied: zipp&gt;=0.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34;-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (3.4.0)\nBuilding wheels for collected packages: en-core-web-sm\n  Building wheel for en-core-web-sm (setup.py): started\n  Building wheel for en-core-web-sm (setup.py): finished with status &#39;done&#39;\n  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047105 sha256=24807959de52b2aa2b5d49620510e215074abfba2931870ee32723b947e854b4\n  Stored in directory: /tmp/pip-ephem-wheel-cache-kakpczga/wheels/b7/0d/f0/7ecae8427c515065d75410989e15e5785dd3975fe06e795cd9\nSuccessfully built en-core-web-sm\nInstalling collected packages: en-core-web-sm\nSuccessfully installed en-core-web-sm-2.3.1\n<span class=\"ansi-green-fg\">✔ Download and installation successful</span>\nYou can now load the model via spacy.load(&#39;en_core_web_sm&#39;)\n<span class=\"ansi-green-fg\">✔ Linking successful</span>\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/en_core_web_sm\n--&gt;\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/spacy/data/en\nYou can now load the model via spacy.load(&#39;en&#39;)\nPython interpreter will be restarted.\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python interpreter will be restarted.\nRequirement already satisfied: spacy in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (2.3.2)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (2.0.4)\nRequirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (1.0.4)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (1.0.4)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (2.22.0)\nRequirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (0.4.1)\nRequirement already satisfied: numpy&gt;=1.15.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (1.18.1)\nRequirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (0.8.0)\nRequirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (1.1.3)\nRequirement already satisfied: thinc==7.4.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (7.4.1)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (3.0.4)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (4.42.1)\nRequirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (1.0.0)\nRequirement already satisfied: setuptools in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy) (45.2.0.post20200210)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (1.25.8)\nRequirement already satisfied: idna&lt;2.9,&gt;=2.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (2.8)\nRequirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (3.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (2020.6.20)\nRequirement already satisfied: importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34; in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy) (3.1.0)\nRequirement already satisfied: zipp&gt;=0.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34;-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy) (3.4.0)\nCollecting en_core_web_sm==2.3.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\nRequirement already satisfied: spacy&lt;2.4.0,&gt;=2.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (2.0.4)\nRequirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.0.4)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.0.4)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (2.22.0)\nRequirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (0.4.1)\nRequirement already satisfied: numpy&gt;=1.15.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.18.1)\nRequirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (0.8.0)\nRequirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.1.3)\nRequirement already satisfied: thinc==7.4.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (7.4.1)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (3.0.4)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (4.42.1)\nRequirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.0.0)\nRequirement already satisfied: setuptools in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (45.2.0.post20200210)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.25.8)\nRequirement already satisfied: idna&lt;2.9,&gt;=2.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (2.8)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (2020.6.20)\nRequirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (3.0.4)\nRequirement already satisfied: importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34; in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (3.1.0)\nRequirement already satisfied: zipp&gt;=0.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34;-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (3.4.0)\nBuilding wheels for collected packages: en-core-web-sm\n  Building wheel for en-core-web-sm (setup.py): started\n  Building wheel for en-core-web-sm (setup.py): finished with status &#39;done&#39;\n  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047105 sha256=24807959de52b2aa2b5d49620510e215074abfba2931870ee32723b947e854b4\n  Stored in directory: /tmp/pip-ephem-wheel-cache-kakpczga/wheels/b7/0d/f0/7ecae8427c515065d75410989e15e5785dd3975fe06e795cd9\nSuccessfully built en-core-web-sm\nInstalling collected packages: en-core-web-sm\nSuccessfully installed en-core-web-sm-2.3.1\n<span class=\"ansi-green-fg\">✔ Download and installation successful</span>\nYou can now load the model via spacy.load(&#39;en_core_web_sm&#39;)\n<span class=\"ansi-green-fg\">✔ Linking successful</span>\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/en_core_web_sm\n--&gt;\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/spacy/data/en\nYou can now load the model via spacy.load(&#39;en&#39;)\nPython interpreter will be restarted.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\nimport spacy\nimport numpy as np\nimport json\nimport re\n#AUTHORS = { 'EAP' : 0, 'HPL' : 1, 'MWS' : 2 }\n\n# Load SpaCy's models\nSPACY = spacy.load('en')\n\n# import data\ndf = pd.read_json('/dbfs/mnt/pobreza/changeanalysis/func-mintic-pobrezamultidimensio/Population and Poverty Mapping/W&J/News_Category_Dataset_v2.json', lines=True)\n\n# Convert the author strings into numbers\n#dataset['author'] = dataset['author'].apply(lambda x: AUTHORS[x])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95910f71-9da2-4550-b49b-4828f949f5bd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Stopwords list from https://github.com/Yoast/YoastSEO.js/blob/develop/src/config/stopwords.js\nstopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1eb748b-c59e-43c9-90b2-b6787fdc6d91"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# removing non alphanumeric character\ndef alpha_num(text):\n    return re.sub(r'[^A-Za-z0-9 ]', '', text)\n\n# removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stopwords:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n  \n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58b03515-025f-4817-aafd-fac355ed0f92"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# apply preprocessing steps\n# connect 'headline' and 'short_description'\ndf['text'] = df['headline'] + \" \" + df['short_description']\ndf['text'] = df['text'].str.lower()\ndf['text'] = df['text'].apply(alpha_num)\ndf['text'] = df['text'].apply(remove_stopwords)\ndf.head()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7231a79e-68f1-4fea-827d-574b5c14629a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category</th>\n      <th>headline</th>\n      <th>authors</th>\n      <th>link</th>\n      <th>short_description</th>\n      <th>date</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CRIME</td>\n      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n      <td>Melissa Jeltsen</td>\n      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n      <td>She left her husband. He killed their children...</td>\n      <td>2018-05-26</td>\n      <td>2 mass shootings texas last week 1 tv left hus...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENTERTAINMENT</td>\n      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n      <td>Andy McDonald</td>\n      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n      <td>Of course it has a song.</td>\n      <td>2018-05-26</td>\n      <td>will smith joins diplo nicky jam 2018 world cu...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ENTERTAINMENT</td>\n      <td>Hugh Grant Marries For The First Time At Age 57</td>\n      <td>Ron Dicker</td>\n      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n      <td>2018-05-26</td>\n      <td>hugh grant marries first time age 57 actor lon...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ENTERTAINMENT</td>\n      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n      <td>Ron Dicker</td>\n      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n      <td>The actor gives Dems an ass-kicking for not fi...</td>\n      <td>2018-05-26</td>\n      <td>jim carrey blasts castrato adam schiff democra...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ENTERTAINMENT</td>\n      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n      <td>Ron Dicker</td>\n      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n      <td>The \"Dietland\" actress said using the bags is ...</td>\n      <td>2018-05-26</td>\n      <td>julianna margulies uses donald trump poop bags...</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":"<div class=\"ansiout\">Out[45]: </div>","removedWidgets":[],"addedWidgets":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category</th>\n      <th>headline</th>\n      <th>authors</th>\n      <th>link</th>\n      <th>short_description</th>\n      <th>date</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CRIME</td>\n      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n      <td>Melissa Jeltsen</td>\n      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n      <td>She left her husband. He killed their children...</td>\n      <td>2018-05-26</td>\n      <td>2 mass shootings texas last week 1 tv left hus...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENTERTAINMENT</td>\n      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n      <td>Andy McDonald</td>\n      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n      <td>Of course it has a song.</td>\n      <td>2018-05-26</td>\n      <td>will smith joins diplo nicky jam 2018 world cu...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ENTERTAINMENT</td>\n      <td>Hugh Grant Marries For The First Time At Age 57</td>\n      <td>Ron Dicker</td>\n      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n      <td>2018-05-26</td>\n      <td>hugh grant marries first time age 57 actor lon...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ENTERTAINMENT</td>\n      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n      <td>Ron Dicker</td>\n      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n      <td>The actor gives Dems an ass-kicking for not fi...</td>\n      <td>2018-05-26</td>\n      <td>jim carrey blasts castrato adam schiff democra...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ENTERTAINMENT</td>\n      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n      <td>Ron Dicker</td>\n      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n      <td>The \"Dietland\" actress said using the bags is ...</td>\n      <td>2018-05-26</td>\n      <td>julianna margulies uses donald trump poop bags...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df = df[[\"category\", \"text\"]]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01e5eb9f-dbed-436b-9019-8eccf25e4bc3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Convert the author strings into numbers\ndf[\"category\"] = df[\"category\"].astype('category')\ndf.dtypes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7a54d63-2316-4ca9-9e88-887a7fd510bf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[47]: category    category\ntext          object\ndtype: object</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[47]: category    category\ntext          object\ndtype: object</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df[\"author\"] = df[\"category\"].cat.codes\ndf.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0c8694b-37ce-4cf3-adbe-69dc05fc7e72"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category</th>\n      <th>text</th>\n      <th>author</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CRIME</td>\n      <td>2 mass shootings texas last week 1 tv left hus...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENTERTAINMENT</td>\n      <td>will smith joins diplo nicky jam 2018 world cu...</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ENTERTAINMENT</td>\n      <td>hugh grant marries first time age 57 actor lon...</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ENTERTAINMENT</td>\n      <td>jim carrey blasts castrato adam schiff democra...</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ENTERTAINMENT</td>\n      <td>julianna margulies uses donald trump poop bags...</td>\n      <td>10</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":"<div class=\"ansiout\">Out[48]: </div>","removedWidgets":[],"addedWidgets":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category</th>\n      <th>text</th>\n      <th>author</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CRIME</td>\n      <td>2 mass shootings texas last week 1 tv left hus...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENTERTAINMENT</td>\n      <td>will smith joins diplo nicky jam 2018 world cu...</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ENTERTAINMENT</td>\n      <td>hugh grant marries first time age 57 actor lon...</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ENTERTAINMENT</td>\n      <td>jim carrey blasts castrato adam schiff democra...</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ENTERTAINMENT</td>\n      <td>julianna margulies uses donald trump poop bags...</td>\n      <td>10</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["dataset = df[[\"author\", \"text\"]]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9fee92b1-3fc9-4f01-92b1-ea301e121b1e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Method"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cec4427c-760b-4b5c-9560-302b43b598f0"}}},{"cell_type":"markdown","source":["What do we have at the beginning of our experiment? From a computational point of view just a massive amount of strings; containing different numbers of sentences and words. In short: A complete nightmare for a machine learning algorithm. To gain valuable knowledge out of it, we have to preprocess it: We need to extract the actual entities like words and punctuation and assign a syntactical meaning to those by detecting their role in the sentence (are they nouns? Or a type of punctuation?). We do not have to implement this \"Tokenization\" and \"Part of Speech - tagging\" ourselves, instead, we will use SpaCy for this task.\nAfter this step, we will have many different sized lists containing numbers representing the actual types of words. By default, machine learning algorithms are unable to deal with such kind of non-fixed length data. But what do we do now?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0bdf5aa4-b650-42ec-b505-393389a9879d"}}},{"cell_type":"code","source":["sentence_lengths = np.fromiter((len(t.split()) for t in dataset['text']), count=len(dataset['text']), dtype='uint16')\n\nprint(\"Minimal sentence length {}: '{}'\".format(\n    np.min(sentence_lengths),\n    dataset['text'][np.argmin(sentence_lengths)]\n))\n\nprint(\"Maximal sentence length\", np.max(sentence_lengths))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d42e0ec-0600-4941-820d-28b02b304e84"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Minimal sentence length 0: &#39;&#39;\nMaximal sentence length 145\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Minimal sentence length 0: &#39;&#39;\nMaximal sentence length 145\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We might think about of filling the lists just with some 0's at the end until they have all the size of the most extensive list. While no information would get lost in such a case, the speed we archive in training of algorithms depends highly on the input size. Should we get a massive amount of 0 just because of a small amount of outliers? Certainly not! As an alternative, we will use a combined approach: We will use a size where 95% of all elements are completely covered, fill shorter ones and truncate the small amount outliers for the sake of training efficiency.\n\nDue to the fact that we are using scikit-learn, we will model this preprocessing as a so called Transformer. Keep in mind that you should in general not rely on global variables like I do with SpaCy in the following; unfortunately, the models are not serializable and will make our usage in further rather painful."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47f5085c-f578-4d6b-a8fb-cd436092967e"}}},{"cell_type":"code","source":["from sklearn.base import BaseEstimator, TransformerMixin\n\nclass PosPreprocessor(BaseEstimator, TransformerMixin):\n    def __init__(self, length_percentile = 95):\n        self.length_percentile = length_percentile\n        self._standartization_factor = 0\n\n    def transform(self, X, *_):\n        assert (self.sentence_size is not None), \"Fitting required\"\n        \n        # Create the output matrix\n        result = np.zeros((len(X), self.sentence_size), dtype='uint8')\n        \n        # Tokenize and POS tag all the documents using multi-threading\n        for i, x in enumerate(SPACY.pipe(X, batch_size=500, n_threads=-1)):\n            # Store the POS-tags\n            tags = np.fromiter((token.pos for token in x), dtype='uint8', count=len(x))\n            \n            # Pad and truncate data, if necessary, and store them in result\n            last_index = len(tags) if len(tags) < self.sentence_size else self.sentence_size\n            result[i, :last_index] = tags[:last_index]\n        \n        # Generate the factor one time to ensure applying the same factor at the next transformations\n        if self._standartization_factor == 0:\n            self._standartization_factor = np.min(result[result != 0]) - 1\n    \n        # Standartize all valid elements to count from 1\n        result[result != 0] -= self._standartization_factor\n        return result\n\n    def fit(self, X, *_):\n        # Define an optimal sentence size covering a specific percent of all sample\n        self.sentence_size = int(np.percentile([len(t.split()) for t in X], self.length_percentile))\n        return self\n    \n    def fit_transform(self, X, *_):\n        self.fit(X)\n        return self.transform(X)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6619c3ea-8a48-4202-b2a5-7fae8b7dbe32"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now we have our set of features and our labels - let's get ready for rumble! Before we start to dive in into Deep Learning, it is reasonable to define a foundation for a further evaluation of the performance.\nFor such cases, RandomForest is often a good candidate for a first prediction. This machine learning algorithm has only a small number of hyperparameter and frequently performs even better than neuronal networks while using only a fraction of the training time."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa06be52-2192-4d2f-a391-933e32863d4e"}}},{"cell_type":"code","source":["from sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\nsyntax_pipeline = Pipeline([\n        ('pre', PosPreprocessor()), \n        ('predictor', RandomForestClassifier(n_estimators=100))\n])\n\n# Get a testing split for further tests\ntest_split = int(0.1 * len(dataset))\n\n# Train the model and evaluate it on former unseen testing data\nsyntax_pipeline.fit(dataset['text'][:test_split], dataset['author'][:test_split])\nsyntax_pipeline.score(dataset['text'][test_split:], dataset['author'][test_split:])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ad00ead-7708-4fb4-b32b-474071958b70"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[52]: 0.133801336519738</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[52]: 0.133801336519738</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Even if the extraction of knowledge is undoubtful a hard task, these results look not that promising. We have to remind ourselves that the probability of having a match just by rolling a dice is 33%. It seems, that the linear way RandomForest and most of the other machine learning algorithms work is unable to get the actual syntax of the sentence. What should we use instead?! \nThe probably most promising solution would be the usage of a specific group of Artificial Neural Networks: Recurrent Neural Networks.\n\nRecurrent neural networks are, no surprise, a specific part of the family of artificial neural networks. Unlike the classical feed-forward neural networks, RNNs were conducted as a more potent tool especially for tasks involving the use of sequences of features rather than only features itself. While former type of network generates a fixed output vector out of a fixed input vector in a fixed number of computational steps, RNNs work on sequences of these vectors. Foundation of this is their ability to have an interior state which gets adapted between different samples and allows therefore further consideration of spatial frequency and common pattern. These capabilities were successfully applied in speech recognition, machine translation and even the generation of text. Instead of the classical, simple RNN neurons, we will evaluate LSTM and GRU neurons in this experiment. Both were designed to face the \"long-term dependency problem\": While RNNs can find dependencies between adjacent elements easily, further context commonly needed in language processing tasks are not possible. \n\nThe most crucial part in using every artificial neuronal network is its actual design determining its final performance. While there are some heuristics for some of its hyperparameter, mostly we have to rely on extensive testing. We delegate the work to scikit-learn by creating the following classifier, which allows an automatization of the training process. No worries - the code might be long, but I explain it afterward."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82a84dd0-c4c8-4ebb-bc5e-6d87c2727ef3"}}},{"cell_type":"code","source":["\npip install keras && python "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e029713e-2744-4137-9e77-05160c5c2b44"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting keras\n  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\nRequirement already satisfied: pyyaml in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from keras) (5.3.1)\nRequirement already satisfied: h5py in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from keras) (2.10.0)\nRequirement already satisfied: scipy&gt;=0.14 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from keras) (1.4.1)\nRequirement already satisfied: numpy&gt;=1.9.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from keras) (1.18.1)\nRequirement already satisfied: six in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from h5py-&gt;keras) (1.14.0)\nInstalling collected packages: keras\nSuccessfully installed keras-2.4.3\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting keras\n  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\nRequirement already satisfied: pyyaml in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from keras) (5.3.1)\nRequirement already satisfied: h5py in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from keras) (2.10.0)\nRequirement already satisfied: scipy&gt;=0.14 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from keras) (1.4.1)\nRequirement already satisfied: numpy&gt;=1.9.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from keras) (1.18.1)\nRequirement already satisfied: six in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages (from h5py-&gt;keras) (1.14.0)\nInstalling collected packages: keras\nSuccessfully installed keras-2.4.3\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Cancelled","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["from sklearn.base import BaseEstimator, ClassifierMixin\nfrom keras.utils import to_categorical\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, GRU, SimpleRNN, Activation, Dropout\n\nclass RnnClassifier(BaseEstimator, ClassifierMixin):\n\n    def __init__(self,\n                 batch_size=32,\n                 epochs=3,\n                 dropout=0,\n                 rnn_type='gru',\n                 hidden_layer=[64, 32]):\n        \n        # How many samples are processed in one training step?\n        self.batch_size = batch_size\n        # How long should the artificial neural network train?\n        self.epochs = epochs\n        # How much dropout do we put into the model to avoid overfitting?\n        self.dropout = dropout\n        # Which type of RNN do we want?\n        self.rnn_type = rnn_type\n        # Do we have hidden layer? If yes, how many which how many neurons?\n        self.hidden_layer = hidden_layer\n        \n        self._rnn = None\n        self._num_classes = None\n        self._num_words = None\n\n    def fit(self, X, Y=None):\n        assert (Y is not None), \"Y is required\"\n        assert (self.rnn_type in ['gru', 'lstm', 'simple']), \"Invalid RNN type\"\n\n        # How many different tags do we have?\n        self._num_words = np.max(X) + 1\n        \n        # How many classes should we predict?\n        self._num_classes = np.max(Y) + 1\n        \n        node_type = None\n        if self.rnn_type is 'gru':\n            node_type = GRU\n        elif self.rnn_type is 'lstm':\n            node_type = LSTM\n        else:\n            node_type = SimpleRNN\n        \n        # Transfer the data into a appropiated shape\n        X = self._reshape_input(X)\n\n        # Ready for rumble?! Here the actual neural network starts!\n        self._rnn = Sequential()\n        self._rnn.add(node_type(X.shape[1], \n                                input_shape=(X.shape[1], self._num_words), \n                                return_sequences=(len(self.hidden_layer) > 0)\n                               ))\n        \n        # Add dropout, if needed        \n        if self.dropout > 0:\n            self._rnn.add(Dropout(self.dropout))\n\n        # Add the hidden layers and their dropout\n        for (i, hidden_neurons) in enumerate(self.hidden_layer):\n            sequences = i != len(self.hidden_layer) - 1\n            \n            self._rnn.add(node_type(hidden_neurons, return_sequences=sequences))\n            if self.dropout > 0:\n                self._rnn.add(Dropout(self.dropout))\n        \n        # Add the output layer and compile the model\n        self._rnn.add(Dense(3, activation='softmax'))\n        self._rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n        # Convert the results in the right format and start the training process\n        Y = to_categorical(Y, num_classes=self._num_classes)\n        self._rnn.fit(X, Y, epochs=self.epochs,\n                      batch_size=self.batch_size,\n                      verbose=0)\n\n        return self\n\n    def predict(self, X, y=None):\n        if self._rnn is None:\n            raise RuntimeError(\"Fitting required before prediction!\")\n        \n        # 'Softmax' returns a list of probabilities - just use the highest onw\n        return np.argmax(\n            self._rnn.predict(\n                self._reshape_input(X), \n                batch_size=self.batch_size\n        ))\n\n    def score(self, X, y=None):\n        assert (y is not None), \"Y is required\"\n\n        # Evaluate the model on training data\n        return self._rnn.evaluate(\n            self._reshape_input(X), \n            to_categorical(y, num_classes=self._num_classes)\n        )[1]\n    \n    def _reshape_input(self, X):\n        result = np.resize(X, (X.shape[0], X.shape[1], self._num_words))\n        for x in range(0, X.shape[0]):\n            for y in range(0, X.shape[1]):\n                 result[x, y] = to_categorical(X[x, y], num_classes=self._num_words)[0]\n        return result"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4da9e997-f505-4863-890a-f4869f546963"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["This is a big chunk of code, isn't it? The actual design of the RNN it thereby rather trivial, most of the code is used to convert the samples from and into the needed format. Instead of dealing with numbers like we as humans would do it, Artifical Neural Networks prefer the so-called binary \"One Hot Encoding.\" Therefore, instead of using  categories \"1\", \"2,\" and \"3\" for the author, we have to use binary vectors of the form \"(1, 0, 0)\", \"(0, 1, 0),\" and \"(0, 0, 1)\" for the output. Following the same logic for the input, we have to add a third dimension, encoding the numbers at a specific sample in a specific position as such a vector.\n\nThe RNN is after all these transformations merely a sequential list of layers. We start with a layer with the size of the sentence we defined earlier; therefore every potential word matches precisely one neuron. Afterwards, a dynamic number of the so-called hidden layer between input and output is added. If specified in the parameters, we add so-called \"Dropout\" between them: Especial RNNs tends to overfit the data rather early and learn therefore more the training data than the actual model behind. Dropout adds a specified amount of random noise to the learning process to avoid this. In the end, we use a simple layer with three neurons and the softmax activation function to gain the final prediction. Each of its neurons will have a float between 0 and 1 describing the likelihood for that class to occur. By just getting the most likely index, we get the class of our entry according to this prediction. \n\nAfter we build this classifier, we are now able to run a GridSearch. This fancy term just describes a systematical evaluation of all the different permutations of a range of parameter we provided to find the one leading to an optimal model. Again, sci-kits pretty simple syntax leads to a straightforward structure."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a051f7f-8254-4938-b126-6e8f4fac9d74"}}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV, ShuffleSplit\nfrom sklearn.pipeline import Pipeline\n\n# Create the pipeline\nsyntax_pipeline = Pipeline([\n        ('pre', PosPreprocessor()), \n        ('rnn', RnnClassifier(batch_size=64))\n])\n\n# Create the grid search with specifying possible parameter\noptimizer = GridSearchCV(syntax_pipeline, {\n    'rnn__rnn_type' : ('lstm', 'gru'), \n    'rnn__hidden_layer' : ([], [64], [64, 32]),\n    'rnn__epochs': (30, 60, 90),\n    'rnn__dropout': (0, 0.15, 0.3)\n}, cv=ShuffleSplit(test_size=0.10, n_splits=1, random_state=0))\n\n# Start the search: That will take some time!\noptimizer.fit(dataset['text'], dataset['author'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4cb0177a-70b7-41b3-821b-93b97f7024b3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nValueError: in user code:\n\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:533 train_step  **\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:1527 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4561 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py:1117 assert_is_compatible_with\n        raise ValueError(&#34;Shapes %s and %s are incompatible&#34; % (self, other))\n\n    ValueError: Shapes (None, 41) and (None, 3) are incompatible\n\n\n  FitFailedWarning)\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nValueError: in user code:\n\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:533 train_step  **\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:1527 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4561 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /local_disk0/.ephemeral_nfs/envs/pythonEnv-6fe0f2c1-321f-44cf-9624-84a4594a16e4/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py:1117 assert_is_compatible_with\n        raise ValueError(&#34;Shapes %s and %s are incompatible&#34; % (self, other))\n\n    ValueError: Shapes (None, 41) and (None, 3) are incompatible\n\n\n  FitFailedWarning)\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["After we train the model with 10% testing data, we might get the following results:\n\n| Rank | Testing Score | Method | Hidden layer | Dropout | Epochs |\n|------|---------------|--------|--------------|---------|--------|\n| 1    | 63.78%        | GRU    | 64           | 30%     | 30     |\n| 2    | 62.33%        | GRU    | 64 + 32      | 15%     | 30     |\n| 3    | 62.17%        | GRU    | -            | 15%     | 30     |\n\nAs we can see, the more simple GRU neurons seems to be perform better than the more complex LSTM's on that kind of problem. Moreover, the number of hidden layers seems not to be crutial. With this evidencene, we are able to re-run the search due to our reduced scope we might have to consider. As we have to find the sweet spot when additional epochs do not longer result in improvement, we will check these parameter in detail:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c583a945-17ec-49c6-a801-2fee751f7d25"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22a10b2f-21b5-4dd5-b376-b23b13e75371"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"stylometry-identify-authors-by-sentence-structure","dashboards":[],"language":"python","widgets":{},"notebookOrigID":3439549302695048}},"nbformat":4,"nbformat_minor":0}
